#!/usr/bin/env python3
"""
Test script pour v√©rifier la quantification FP8 avec vLLM
"""
import sys
import torch
from pathlib import Path

def test_fp8_quantization():
    """Test la quantification FP8 avec vLLM"""
    
    print("üß™ Test de la quantification FP8 avec vLLM")
    print("=" * 50)
    
    # V√©rifier la compatibilit√© GPU
    if not torch.cuda.is_available():
        print("‚ùå CUDA non disponible")
        return False
    
    gpu_name = torch.cuda.get_device_name(0)
    compute_capability = torch.cuda.get_device_capability(0)
    print(f"üîß GPU: {gpu_name}")
    print(f"üîß Compute Capability: {compute_capability[0]}.{compute_capability[1]}")
    
    # V√©rifier si FP8 est support√© (compute capability >= 8.9)
    if compute_capability[0] < 8 or (compute_capability[0] == 8 and compute_capability[1] < 9):
        print("‚ö†Ô∏è  Attention: FP8 n√©cessite compute capability >= 8.9")
        print("   Votre GPU utilisera probablement W8A16 (poids FP8, activations FP16)")
    else:
        print("‚úÖ GPU compatible FP8 W8A8")
    
    # Test d'importation vLLM
    try:
        from vllm import LLM
        print("‚úÖ vLLM import√© avec succ√®s")
    except ImportError as e:
        print(f"‚ùå Erreur d'importation vLLM: {e}")
        return False
    
    # Test de mod√®le simple avec quantification FP8
    try:
        print("\nüöÄ Test de quantification FP8...")
        
        # Utiliser un mod√®le tr√®s petit pour le test
        test_model = "microsoft/DialoGPT-small"  # ~117MB
        
        llm = LLM(
            model=test_model,
            quantization="fp8",
            gpu_memory_utilization=0.3,
            max_model_len=512,
            dtype="auto"
        )
        
        print("‚úÖ Mod√®le charg√© avec quantification FP8")
        
        # Test de g√©n√©ration simple
        outputs = llm.generate(["Hello"], sampling_params={"max_tokens": 5, "temperature": 0.8})
        print(f"‚úÖ G√©n√©ration test r√©ussie: {outputs[0].outputs[0].text}")
        
        # Nettoyage
        del llm
        torch.cuda.empty_cache()
        
        return True
        
    except Exception as e:
        print(f"‚ùå Erreur lors du test FP8: {e}")
        return False

def test_model_loading():
    """Test le chargement du mod√®le principal"""
    
    print("\nüîç V√©rification du mod√®le principal...")
    
    # V√©rifier si le mod√®le SFT existe
    sft_model_path = Path("results/step-1_SFT_fused")
    if sft_model_path.exists():
        print(f"‚úÖ Mod√®le SFT trouv√©: {sft_model_path}")
        
        # Lister les fichiers du mod√®le
        model_files = list(sft_model_path.glob("*.bin")) + list(sft_model_path.glob("*.safetensors"))
        if model_files:
            total_size = sum(f.stat().st_size for f in model_files) / (1024**3)
            print(f"üìä Taille du mod√®le: {total_size:.1f} GB")
        
        return True
    else:
        print(f"‚ùå Mod√®le SFT non trouv√©: {sft_model_path}")
        return False

def main():
    """Fonction principale"""
    
    print("üß™ Tests de Quantification FP8 pour RLT")
    print("=" * 60)
    
    # Test 1: Compatibilit√© FP8
    fp8_ok = test_fp8_quantization()
    
    # Test 2: Mod√®le principal
    model_ok = test_model_loading()
    
    print("\nüìã R√©sum√© des tests:")
    print(f"   FP8 Quantization: {'‚úÖ' if fp8_ok else '‚ùå'}")
    print(f"   Mod√®le principal: {'‚úÖ' if model_ok else '‚ùå'}")
    
    if fp8_ok and model_ok:
        print("\nüéâ Tous les tests r√©ussis ! Vous pouvez utiliser FP8.")
        print("\nüí° Configuration recommand√©e:")
        print("   - vllm_quantization: fp8")
        print("   - vllm_gpu_memory_utilization: 0.4")
        print("   - vllm_max_model_len: 8192")
    else:
        print("\n‚ö†Ô∏è  Certains tests ont √©chou√©. V√©rifiez les erreurs ci-dessus.")
    
    return fp8_ok and model_ok

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1) 