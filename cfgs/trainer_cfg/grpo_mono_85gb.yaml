defaults:
  - base
  - reward_cfg@_global_: teacher_logprob_kl
  - _self_

trainer_log_name: grpo_mono_85gb

artificial_epochs: 1

# Optimized parameters for 85GB VRAM (10GB safety margin)
max_steps: 200
train_batch_size: 96      # Reduced from 128 for safety
per_device_train_batch_size: 3  # Reduced from 4

num_generations: 24       # Reduced from 32

learning_rate: 0.000001
beta: 0.04

model_init_kwargs: null
remove_unused_columns: false

# Adjusted context windows for 85GB
max_prompt_length: 6144   # Reduced from 8192
max_completion_length: 6144  # Reduced from 8192

shuffle_generation_inputs: true

# Disable DeepSpeed specific features (still mono-GPU)
ds3_gather_for_generation: false

temperature: 1.0
top_p: 1.0
top_k: ~
min_p: ~
repetition_penalty: 1.0

generation_aggregation_steps: 3  # Reduced from 4

# vLLM settings optimized for 85GB with safety margin
use_vllm: true
vllm_device: auto
vllm_gpu_memory_utilization: 0.55  # More conservative (was 0.6)
vllm_dtype: auto
vllm_max_model_len: 6144   # Match context length

# Keep Ray disabled (mono-GPU)
use_ray: false
ray_share_training_devices: false
ray_tensor_parallelism: 1
ray_data_parallelism: null
ray_no_memory_duplication: false
vllm_sleep_level: 0
enable_prefix_caching: true
enforce_eager: false

# Disable vLLM server (use local vLLM)
use_vllm_server: false
vllm_host: null
vllm_port: null
vllm_group_port: 51216
num_vllm_clients: 1

reward_weights: null

sync_ref_model: false
ref_model_mixup_alpha: 0.9
ref_model_sync_steps: 64

# Optimized accumulation for 85GB
backprop_accumulation_steps: 2
backprop_accumulation_micro_batch_size: 2
offload_untrained_models: false  # Keep all models on GPU with 85GB!
unbias_log_probabilities: true

log_completions: true
save_completions_probability: 0.08  # Slightly reduced

push_to_hub: false

activate_debugging_logs: true

# Teacher-specific settings
logging_prob: 0.1
student_model: null
use_reference_teacher_model: false
student_model_init_kwargs: null
completion_only_training: false
disable_student_offloading: false

# Reward function configuration
reward_fns:
  _target_: hydra_utils.wrap_as_list
  teacher_reward: ${teacher_reward}

trainer_args:
  _target_: trainers.GRPOConfig
  model_init_kwargs: ${model_init_kwargs}
  remove_unused_columns: ${remove_unused_columns}
  max_prompt_length: ${max_prompt_length}
  num_generations: ${num_generations}
  max_completion_length: ${max_completion_length}
  shuffle_generation_inputs: ${shuffle_generation_inputs}
  ds3_gather_for_generation: ${ds3_gather_for_generation}
  temperature: ${temperature}
  top_p: ${top_p}
  top_k: ${top_k}
  min_p: ${min_p}
  repetition_penalty: ${repetition_penalty}
  generation_aggregation_steps: ${generation_aggregation_steps}
  use_vllm: ${use_vllm}
  vllm_device: ${vllm_device}
  vllm_gpu_memory_utilization: ${vllm_gpu_memory_utilization}
  vllm_dtype: ${vllm_dtype}
  vllm_max_model_len: ${vllm_max_model_len}
  use_ray: ${use_ray}
  ray_share_training_devices: ${ray_share_training_devices}
  ray_tensor_parallelism: ${ray_tensor_parallelism}
  ray_data_parallelism: ${ray_data_parallelism}
  ray_no_memory_duplication: ${ray_no_memory_duplication}
  enable_prefix_caching: ${enable_prefix_caching}
  enforce_eager: ${enforce_eager}
  vllm_sleep_level: ${vllm_sleep_level}
  use_vllm_server: ${use_vllm_server}
  vllm_host: ${vllm_host}
  vllm_port: ${vllm_port}
  vllm_group_port: ${vllm_group_port}
  num_vllm_clients: ${num_vllm_clients}
  learning_rate: ${learning_rate}
  beta: ${beta}
  reward_weights: ${reward_weights}
  sync_ref_model: ${sync_ref_model}
  ref_model_mixup_alpha: ${ref_model_mixup_alpha}
  ref_model_sync_steps: ${ref_model_sync_steps}
  log_completions: ${log_completions}
  save_completions_probability: ${save_completions_probability}
  artificial_epochs: ${artificial_epochs}
  backprop_accumulation_steps: ${backprop_accumulation_steps}
  backprop_accumulation_micro_batch_size: ${backprop_accumulation_micro_batch_size}
  offload_untrained_models: ${offload_untrained_models}
  unbias_log_probabilities: ${unbias_log_probabilities}
  activate_debugging_logs: ${activate_debugging_logs}
  push_to_hub: ${push_to_hub}

trainer:
  _target_: trainers.TeacherGRPOTrainer
  student_model: ${student_model}
  use_reference_teacher_model: ${use_reference_teacher_model}
  student_model_init_kwargs: ${student_model_init_kwargs}
  logging_prob: ${logging_prob}
  disable_student_offloading: ${disable_student_offloading}
  reward_funcs: ${reward_fns} 